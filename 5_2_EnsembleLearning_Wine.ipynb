{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOfcYf4ULDIqjDFa9Fq7vh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kitagawa-Mariin/HenryPark_Python/blob/Machine_Learning/5_2_EnsembleLearning_Wine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Random Forest*** \\\n",
        "\n",
        "Create forest with randomly created decision trees. It creates final prediction from the predictions of each decision trees. They create data randomly from our input samples. \\\n",
        "\n",
        "\n",
        "***Bootstrap Sample***\n",
        "1. in 1000 samples, get 1 first, put it back to the bag.\n",
        "2. pick samples with step 1 as same size of input data.\n",
        "\n",
        "Before training decision trees, pick several features randomly and separate nodes. RandomForestClassifier select squared rooted number of entire feature numbers. So, if it has 4 features, each node uses two features.(For regressor, it uses every features) /\n",
        "\n",
        "SKlearn train 100 decision trees for random forest. For classification, they get the mean value of probabilities of each classes from every decision trees and the highest value is the prediction. \\\n",
        "\n",
        "For regression, they get the mean value of each predictions."
      ],
      "metadata": {
        "id": "6t8Zib5e4CxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctQuJlF_oUye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69d16cb-864c-4911-f46e-c3496301e1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n",
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_) #랜덤 포레스트가 특성의 일부를 램덤하게 선택하여 결정 트리를 훈련하기 때문에, 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻습니다.\n",
        "#reduce overfitting,increase generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Out of Bag samples*** \\\n",
        "\n",
        "Remaining samples that are not in bootstrap samples >> evaluate the model with these samples like validate samples."
      ],
      "metadata": {
        "id": "o87vm9OPFAZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score = True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adZWd-KOCYST",
        "outputId": "dc8e930b-3d03-447e-a438-c2e49cb2c413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extra Trees*** \\\n",
        "\n",
        "Train 100 decision trees. Similiar with RandomForest. But it does not use bootstrap samples. They use entire training set. BUT, they **RANDOMLY split the node**, not with the best ways like decision tree. \\\n",
        "\n",
        "하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다."
      ],
      "metadata": {
        "id": "xyvcc6PiYeH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier #ExtraTreesRegressor for regression.\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "id": "HxgpNXlUFWAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efd32b0-0804-48b8-be08-4ba2a7f200bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVUj4YF5aBQ9",
        "outputId": "d5e52393-3dbc-4a4d-be7c-073408de566b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Gradient Boosting*** \\\n",
        "\n",
        "깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완. 깊이가 3인 결정 트리 100개 사용. 깊이가 얕기 때문에, 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대. \\\n",
        "\n",
        "classifier : logistic log loss \\\n",
        "regressor : 평균 제곱 오차 함수 mean square error \\\n",
        "\n",
        "경사 하강법처럼 가장 낮은 곳을 찾아 천천히 내려감. 깊이가 얕은 트리를 사용하여, 천천히 내려감."
      ],
      "metadata": {
        "id": "EdkIV_xQbHmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb = GradientBoostingClassifier(random_state = 42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#gradient boosting은 결정 트리의 개수를 늘려도 과대적합에 매우 강함.\n",
        "#학습률(learning rate)을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICKWcrg0ahwK",
        "outputId": "618a6f03-e802-470a-d808-4c6d8db19a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#learning_rate의 기본값은 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSijpTkjchW_",
        "outputId": "e6a66618-6871-4275-8a0e-a64d91ea8f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)\n",
        "#트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample : 기본값 1.0 전체 훈련 세트.\n",
        "#1.0보다 작다면 훈련 세트의 일부.\n",
        "#그레디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능 하지만 순서대로트리를 추가하기 때문에 훈련 속도가 느림.\n",
        "#GradientBoostingClassifier에는 n_jobs 매개변수가 없다.\n",
        "#GradientBoostingRegressor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBgsnNc7d90I",
        "outputId": "fcd00b73-6528-4565-cdd6-0dc61b79a8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Histrogram-based Gradient Boosting*** \\\n",
        "\n",
        "시간 성능을 높인 gradient boosting \\\n",
        "\n",
        "입력 특성을 256개의 구간으로 나눔(훈련 데이터를 256개 정수구간으로 나눔). 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다. \\\n",
        "히스토그램 기반 그레디언트 부스팅은 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용.\\\n",
        "입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다. \\\n",
        "\n"
      ],
      "metadata": {
        "id": "CLLQV11ie0d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#n_estimators 대신 부스팅 반복 횟수를 지정하는 max_iter를 사용. 성능을 높이려면 max_iter 매개변수를 테스트."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f6_J89HeMu4",
        "outputId": "5e306601-9427-4a7c-b041-220458a1ede4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "permutation_importance(). 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지 관차랗여 어떤 특성이 중요한지 계산. \\\n",
        "\n",
        "훈련 세트뿐만 아니라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델에 모두 사용가능. \\\n",
        "\n",
        "n_repeats 매개변수는 랜덤하게 섞을 횟수.\n"
      ],
      "metadata": {
        "id": "DmH-FCiMgP8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state = 42, n_jobs = -1)\n",
        "print(result.importances_mean)\n",
        "#importances\n",
        "#importances_mean\n",
        "#impotances_std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYqFjTtff7H5",
        "outputId": "3d2268f6-45bf-4c96-fcdb-2dc8bfebbb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state = 42, n_jobs = -1)\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAt4-DShgx74",
        "outputId": "7a92549d-dd80-4b9e-b27f-298aa9161009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hgb.score(test_input, test_target)\n",
        "#HistGradientBoostingRegressor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CH26hSYhG_D",
        "outputId": "dae59052-b9f2-49bb-de4d-a75978441026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use XGBoost class for boosting\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaZRwrehhPV_",
        "outputId": "6f424df2-c9c4-4e29-a94e-9f9848458887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9558403027491312 0.8782000074035686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Also LightGBM\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)"
      ],
      "metadata": {
        "id": "uNI8BP9kh3Tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}